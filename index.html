<!DOCTYPE html>
<html>

<head>
  <title>Double Y: Building Extraction Generalization</title>
  <link rel="icon" type="image/png" href="static/images/orbit.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <meta name="description" content="Double Y: Building Extraction Generalization">
  <meta property="og:title" content="Double Y: Building Extraction Generalization"/>
  <meta property="og:description" content="ENHANCING CROSS-CITY GENERALIZABILITY OF BUILDING SEGMENTATION MODEL"/>
  <meta property="og:url" content="https://github.com/DoubleY-BEGC2024"/>
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Enhancing Cross-City Building Extraction: From More Data to Diffusion-Augmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/wongyijie/" target="_blank">Yi Jie WONG</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yinloonkhor/" target="_blank">Yin Loon KHOR</a></span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><strong>Group Name:</strong> Double-Y | <strong>Public
                  Leaderboard:</strong> 1st out of 68 Entrants <br>
                <a href="https://www.kaggle.com/competitions/building-extraction-generalization-2024/overview">IEEE BigData Cup 2024: Building
                Extraction Generalization Challenge</a>
              </span>

              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- PDF -->
                <span class="link-block">
                  <a href="static/pdfs/Team Double Y - Approach Document.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                
                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Double-Y-EY-Challenge-2024/EY-challenge-2024" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Source Code</span>
                  </a>
                </span>

                <!-- Best model -->
                <span class="link-block">
                  <a href="https://github.com/Double-Y-EY-Challenge-2024/EY-challenge-2024/blob/main/best-trained-model.pt"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Best Model</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Teaser GIF -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/heatmaps/movie.gif" alt="Inference samples predicted by our trained model.">
      <h2 class="subtitle">
        Geospatial analysis.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser GIF -->



  <!-- Summary -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Summary</h2>
          <div class="content has-text-justified">
            <p> This competition embarks on this challenge by utilizing a building footprint dataset from the Tokyo area as the primary 
              training set, with plans to extend testing to other Japanese regions. This approach aims to inspire the development of models 
              with robust generalization capabilities, capable of overcoming the hurdles of automatic building footprint detection and 
              extraction across various landscapes. Overcoming this challenge signifies the creation of a novel approach for efficient, 
              cost-effective, and precise building footprint extraction at a national level with minimal regional data, showcasing its 
              potential applicability worldwide.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Competition Overview -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Competition Overview</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p><strong>1. Objective:</strong> The goal of the challenge is to develop a machine learning model to identify and detect 
            “damaged” and “un-damaged” coastal infrastructure (residential and commercial buildings), which have been impacted by 
            natural calamities such as hurricanes, cyclones, etc. Participants will be given pre- and post-cyclone satellite images 
            of a site impacted by Hurricane Maria in 2017 and build a machine learning model, designed to detect four different objects 
            in a satellite image of a cyclone impacted area:</p>
          <ul>
            <li>Undamaged residential buildings</li>
            <li>Damaged residential buildings</li>
            <li>Undamaged commercial buildings</li>
            <li>Damaged commercial buildings</li>
          </ul>
          <p><strong>2. Mandatory Dataset:</strong></p>
          <ul>
            <li>High-resolution panchromatic satellite images before and after a tropical cyclone: Maxar GeoEye-1
              (optical)</li>
          </ul>
          <p><strong>3. Optional Dataset (that we used):</strong></p>
          <ul>
            <li><a href="https://planetarycomputer.microsoft.com/dataset/ms-buildings">Microsoft Building footprint dataset</a>, 
              with over 999 million buildings from Bing Maps imagery between 2014 and 2021 including Maxar and Airbus 
              imagery.</li>
          </ul>          
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Competition Overview -->

  <!-- Key Challenges -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Key Challenges</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Dataset Collection:</strong> Manually annotating all four classes in the provided high-resolution
            satellite dataset from Maxar's GEO-1 mission, covering an area of 327 sq.km of San Juan, Puerto Rico, is a
            time-consuming task. With only one month for the competition duration, this task poses significant
            challenges in terms of time and energy allocation.
          </p>
          <p>
            <strong>2. Class Imbalanced:</strong> The dataset contains four unique classes. However, our analysis 
            indicates that damaged buildings are significantly underrepresented compared to undamaged ones. Moreover, 
            residential buildings are more prevalent than commercial ones. This imbalance may introduce bias into the model, 
            causing it to favor the majority class.
          </p>
          <p>
            <strong>3. Out-of-Distribution data:</strong> We noticed that the competition’s validation dataset comprises only 
            buildings from rural settings. However, the training dataset consists of a mixture of images from rural settings, 
            industrial zones, and urban areas. Our empirical study reveals that mixing images from non-rural settings can have 
            a severe impact on model learning.
          </p>          
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Key Challenges -->

  <!-- Key elements and Assumptions -->  
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Model Selection</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>YOLOv8 series comes with several instance segmentation models, ranging from the smallest nano (n) variant to the 
            largest extra-large (x) variant. We performed several experiments to select the best YOLOv8 variant for our task, 
            considering both the F1 score and model complexity, as shown in Table I. Additionally, we compared the performance of 
            YOLOv8-based instance segmentation models with other state-of-the-art models, including YOLOv9, Mask R-CNN, and EfficientNet. 
            All models are trained for 50 epochs with 640 image size. During test time and submission, the confidence and NMS IoU 
            thresholds are set as 0.20 and 0.70, unless stated otherwise. We also tested the F1-score of the models with a confidence 
            threshold of 0.50, primarily to evaluate how confident the models are rather than for actual submission.
          </p>
          <table class="table is-bordered is-hoverable">          
            <thead>
              <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Pretrained Weights</th>
                <th rowspan="2">Batch Size</th>
                <th rowspan="2">Params (M)</th>
                <th rowspan="2">FLOPs (G)</th>
                <th colspan="2">Public F1-Score</th>
              </tr>
              <tr>
                <th>Conf = 0.50</th>
                <th>Conf = 0.20</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>YOLOv8n-seg</td>
                <td rowspan="4">DOTAv1 Aerial Detection</td>
                <td>16</td>
                <td>3.4</td>
                <td>12.6</td>
                <td>0.510</td>
                <td>0.645</td>
              </tr>
              <tr>
                <td>YOLOv8s-seg</td>
                <td>16</td>
                <td>11.8</td>
                <td>42.6</td>
                <td>0.535</td>
                <td>0.654</td>
              </tr>
              <tr>
                <td>YOLOv8m-seg</td>
                <td>16</td>
                <td>27.3</td>
                <td>110.2</td>
                <td>0.592</td>
                <td>0.649</td>
              </tr>
              <tr>
                <td>YOLOv8x-seg</td>      
                <td>8</td>
                <td>71.8</td>
                <td>344.1</td>
                <td>0.579</td>
                <td>0.627</td>
              </tr>
              <tr>
                <td>YOLOv9c-seg</td>
                <td rowspan="2">COCO Segmentation</td>
                <td>4</td>
                <td>27.9</td>
                <td>159.4</td>
                <td>0.476</td>
                <td>0.577</td>
              </tr>
              <tr>
                <td>Mask R-CNN (MPViT-Tiny)</td>
                <td>4</td>
                <td>17</td>
                <td>196.0</td>
                <td>-</td>
                <td>0.596</td>
              </tr>
              <tr>
                <td>EfficientNet-b0-YOLO-seg</td>
                <td>ImageNet</td>
                <td>4</td>
                <td>6.4</td>
                <td>12.5</td>
                <td>-</td>
                <td>0.560</td>
              </tr>
            </tbody>
          </table>
          
          <p>Our observations:</p>
          <ol type="1" padding-left: 0;">
            <li style="margin-bottom: 5px;">Generally, we observe that the F1 score increases when scaling up the model from 
              the smallest YOLOv8n-seg to the medium size YOLOv8m-seg.  Notably, there is a significant jump in F1 score 
              from the YOLOv8s-seg to the YOLOv8m-seg when evaluated in confidence threshold of 0.50, with the score improving 
              from 0.535 to 0.592. </li>
            <li style="margin-bottom: 5px;">Interestingly, the F1 score of YOLOv8m-seg is slightly lower than the smaller YOLOv8s-seg 
              when setting the confidence threshold to 0.20. This observation suggest that the m-variant still has a bigger room of 
              improvement compared to s-variant. </li>
            <li style="margin-bottom: 5px;">Meanwhile, the largest YOLOv8x-seg variant has a lower F1 score than YOLOv8m-seg in 
              both confidence threshold 0.50 and 0.20. This suggests that further improvements in F1 score beyond the m-variant 
              may be minimal unless we enhance the quality of the training dataset or address generalizability issues.</li>
            <li style="margin-bottom: 5px;"> We also tried training YOLOv9 instance segmentation model. Specficially, we chose YOLOv9c, 
              which corresponds to the m-variant of YOLOv8 (YOLOv8m-seg). However, we find that YOLOv9 is hard to train and slower 
              due to the high FLOPS, with no F1-score improvement.</li>
            <li style="margin-bottom: 5px;">Other than YOLO family, we also tried using Mask R-CNN with the MPViT backbone. However, 
              due to resource constraints, we were only able to test the smallest MPViT-Tiny for our Mask R-CNN backbone. The modified
              Mask R-CNN has a slightly higher F1 score than YOLOv9c, but is inferior to all YOLOv8 variants we tried. It is not only
              slow, but also not accurate.</li>            
          </ol>
          
        </div>
      </div>
    </div>
  </div>
  <!-- Key elements and Assumptions -->   
  
  <!-- Methodology Overview -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Methodology Overview</h2>
      <div class="content has-text-justified">
        <div style="text-align: center;">
          <img src="static/images/Team Double Y - Methodology.jpg" alt="PrepareData" width="820">
          <p class="caption" style="width: 100%; text-align: center;"><b>Figure 1. Overview of the proposed methodology.</b><br>
            Workflow illustrating the complete process from data acquisition to model training.</p>
        </div>
      </div>
      <br>
    </div>
  </div>
  <!-- End methodology overview--> 

  
  <!-- Key Highlights of our Pipeline -->
  <!--   
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Key Highlights</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Dataset Collection:</strong> We remove images that only feature undamaged residential buildings 
            (the majority class) to ensure the training dataset does not skew towards one class over another.
          </p>
          <p>
            <strong>2. Class Imbalanced:</strong> We removed images that only feature undamaged residential buildings 
            (the majority class) to ensure the training dataset does not skew towards one class over another.
          </p>
          <p>
            <strong>3. Out-of-Distribution data:</strong> We removed images featuring industrial zones and urban areas, 
            as the validation dataset primarily consists of rural settings. 
          </p>          
        </div>
      </div>
      <br><br>
    </div>
  </div>
  -->  
  <!-- Key Highlights of our Pipeline -->

  
  <!-- Model -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Model</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            The goal of Phase 1 is to identify and detect “damaged” and “undamaged” coastal infrastructure, which is an  
            object detection task. To tackle this challenge, our team has opted for Ultralytics <strong>YOLOv8</strong>, 
            one of the state-of-the-art (SOTA) object detection models renowned for its speed and accuracy. Despite the availability 
            of competitors like YOLOv9, we prefer Ultralytics YOLOv8 for its user-friendliness and well-documented workflows 
            that streamline training and deployment. We choose the smallest YOLOv8 - <strong>YOLOv8n</strong>, since it is 
            <strong>unwise to use larger model when dealing with limited dataset</strong>, as it may lead to overfitting. 
            Given more time, we would explore other YOLOv8 version and other SOTA models when we have a bigger dataset. 
            Meanwhile, our empirical study revealed that the main influencing factor on the detection accuracy is 
            the quantity and quality of the annotated dataset. Hence, we argue that the main focus of the challenge 
            should be data annotation. We provide details on how we built our training dataset in the next section.            
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Model -->

  
  <!-- Submission Experiments Table -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Submission Experiment</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>We conducted a comprehensive series of experiments, submitting a total of 30 entries. Here are select
            highlights:</p>
          <table class="table is-bordered is-hoverable">
            <thead>
              <tr>
                <th>Setup</th>
                <th>Pretraining</th>
                <th>Crowdsourced Dataset</th>
                <th>Expert Dataset</th>
                <th>MLOps</th>
                <th>mAP</th>                      
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>A</td>
                <td>&#x2713;</td>
                <td></td>
                <td></td>
                <td></td>              
                <td>0.10</td>
              </tr>
              <tr>
                <td>B</td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>
                <td></td>
                <td></td>              
                <td>0.44</td>
              </tr>
              <tr>
                <td>C</td>
                <td>&#x2713;</td>
                <td></td>
                <td>&#x2713;</td>
                <td></td>              
                <td>0.39</td>
              </tr>
              <tr>
                <td>D</td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>
                <td></td>              
                <td>0.50</td>
              </tr>
              <tr>
                <td>E</td>
                <td></td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>
                <td></td>              
                <td>0.24</td>
              </tr>
              <tr>
                <td>F</td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>
                <td>&#x2713;</td>              
                <td><b>0.51</b></td>
              </tr>                 
            </tbody>
          </table>

          <!--
          <p style="font-size: 12px;">Note: Class Ratio (Undamaged Residential Building : Damaged Residential Building :
            Undamaged Commercial Building : Damaged Commercial Building)</p>
          -->
          <p>Further experiments were conducted, including:</p>
          <ul style="list-style-type: none; padding-left: 0;">
            <li style="margin-bottom: 5px;"><strong>Setup A:</strong> 0.10 - We pretrained a YOLOv8n model using the 
              Puerto Rico dataset. Surprisingly, we achieved a mAP of 0.10 on the EY validation dataset, without any 
              manual annotation from our side!</li>
            <li style="margin-bottom: 5px;"><strong>Setup B:</strong> 0.44 - When fine-tuning the pretrained model on 
              the crowd-sourced dataset, we achieved an mAP of 0.44, which exceeds the completion threshold for this 
              challenge (mAP 0.40). </li>
            <li style="margin-bottom: 5px;"><strong>Setup C:</strong> 0.39 - When fine-tuning the pretrained model 
              directly on the Expert dataset, we can achieve an mAP of 0.39, despite the dataset containing only 28 
              unique data (84 after augmentation). This shows that the quality of data is equally important, if not 
              more important than the quantity of data.</li>
            <li style="margin-bottom: 5px;"><strong>Setup D:</strong> 0.50 - We initially fine-tune the pretrained 
              model using a large-scale crowd-sourced dataset to quickly warm it up. Subsequently, we fine-tune the 
              model on the expert dataset, which has more accurate labels. With this approach, we achieved a mAP of 
              0.50.</li>
            <li style="margin-bottom: 5px;"><strong>Setup E:</strong> 0.24 - We demonstrate that without pretraining, 
              the performance is not satisfying even when both the crowd-sourced and expert datasets are utilised, 
              only achieving mAP 0.24.</li>
            <li style="margin-bottom: 5px;"><strong>Setup F:</strong> 0.51 - Finally, we demonstrate that by employing 
              the proposed MLOps cycle, we can enhance the model’s mAP to 0.51. Notably, the sole human intervention 
              in this MLOps cycle involves verifying the self-labelled data using the baseline model from Setup E.</li>         
          </ul>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- End of Submission Experiments Table -->

  <!-- Conclusion -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Key Takeaways</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Dataset quality is what you need:</strong> There are 2 observations from our study. Firstly, 
            data quality is as important as data quantity. Secondly, having annotators with expertise in building damage 
            assessment is crucial for producing the high-quality 'expert dataset.' On the contrary, non-experts tend to 
            generate a lower quality dataset, which we refer to as a 'crowdsourced dataset.' However, a high-quality dataset 
            tends to be smaller in size because it takes time to carefully annotate the data. Conversely, a high-quantity 
            dataset tends to have lower quality due to a lack of expertise and attention. This mirrors a real-world scenario 
            of a quality-quantity tradeoff. Fortunately, we found that we can combine the strengths of both datasets, as 
            demonstrated in Setup D from our ablation study in Table I. This involves fine-tuning the pretrained model on 
            the crowdsourced dataset, followed by fine-tuning on the expert dataset.
          </p>
          <p>
            <strong>2. Start with a small model:</strong> We recommend starting with a smaller model. It is unwise to use a 
            larger model when dealing with a limited dataset, as it may lead to overfitting. Our empirical study agrees with 
            this hypothesis, as we failed to achieve a high mAP score using the bigger YOLOv8 version. Given more time, 
            we would explore the bigger YOLOv8 version and other state-of-the-art (SOTA) models when we have a larger dataset.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Conclusion -->

  <!-- Logo Acknowledgment -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Technological Stack</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <a href="https://github.com/ultralytics/ultralytics" target="_blank"><img
                src="static/icons/ultralyticsyolo-logo.svg" alt="ultralytics" style="width: 200px;"></a>
            <a href="https://roboflow.com/" target="_blank"><img
                src="static/icons/roboflow-logo.png" alt="roboflow" style="width: 200px;"></a>          
            <a href="https://pytorch.org/" target="_blank"><img src="static/icons/pytorch-logo.svg" alt="pytorch"
                style="width: 210px;"></a>        
            <a href="https://jupyter.org/" target="_blank"><img src="static/icons/jupyter-logo.png" alt="jupyter"
                style="width: 200px;"></a>              
            <a href="https://www.python.org/" target="_blank"><img src="static/icons/python-logo.svg" alt="python"
                style="width: 200px;"></a>  
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Logo Acknowledgment -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              Special thanks to <a href="https://ey-groupie2024wg.github.io/">EY Groupie-WG</a> for their well-documented
              methodology report. Please feel free to visit their report to see their proposed approach as well!
            </p>
            
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
            </p>

            <p>            
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Default Statcounter code for EY project website -->
  <!-- 
  <script type="text/javascript">
    var sc_project = 12976265;
    var sc_invisible = 1;
    var sc_security = "c70be6f1"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12976265/0/c70be6f1/1/" alt="Web Analytics"
          referrerPolicy="no-referrer-when-downgrade"></a></div>
  </noscript>
  -->
  <!-- End of Statcounter Code -->

</body>

</html>
